import streamlit as st
import json
import re
from datetime import datetime, timedelta
from collections import Counter
import pandas as pd
import random
import sqlite3
from pathlib import Path



# ç®€åŒ–ç‰ˆåˆ†è¯å’Œè¯æ€§æ ‡æ³¨ï¼ˆä¸ä¾èµ–spaCyï¼‰
def simple_tokenize(text):
    """ç®€å•åˆ†è¯ï¼šæå–æ‰€æœ‰å¾·è¯­å•è¯ï¼Œä¿ç•™å¤§å°å†™"""
    tokens = re.findall(r'\b[a-zA-ZÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]+\b', text)
    return tokens

def get_concordance(word, text, window=5):
    """æå–è¯æ±‡çš„ä¸Šä¸‹æ–‡"""
    tokens = simple_tokenize(text)
    concordances = []
    for i, token in enumerate(tokens):
        if token == word:  # ç²¾ç¡®åŒ¹é…ï¼Œä¿ç•™å¤§å°å†™
            start = max(0, i - window)
            end = min(len(tokens), i + window + 1)
            context = ' '.join(tokens[start:end])
            concordances.append(context)
    return concordances[:3]  # æœ€å¤šè¿”å›3ä¸ªä¾‹å¥

def extract_readable_text(text, target_tokens, max_length=None):
    """
    æå–åŒ…å«ç›®æ ‡tokençš„æœ€å¤§è¿ç»­æ–‡æœ¬ç‰‡æ®µ
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        target_tokens: ç›®æ ‡è¯æ±‡é›†åˆï¼ˆç”Ÿè¯+ç†Ÿè¯ï¼‰ï¼Œä¿ç•™å¤§å°å†™
        max_length: æœ€å¤§è¿”å›é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    
    Returns:
        åŒ…å«æœ€å¤šç›®æ ‡è¯æ±‡çš„æ–‡æœ¬ç‰‡æ®µ
    """
    sentences = re.split(r'[.!?]+', text)
    
    best_segment = ""
    max_target_count = 0
    
    # å°è¯•ä¸åŒé•¿åº¦çš„å¥å­ç»„åˆ
    for start_idx in range(len(sentences)):
        current_segment = ""
        current_target_count = 0
        
        for end_idx in range(start_idx, len(sentences)):
            current_segment += sentences[end_idx] + ". "
            
            # å¦‚æœè¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œåœæ­¢
            if max_length and len(current_segment) > max_length:
                break
            
            # ç»Ÿè®¡å½“å‰ç‰‡æ®µä¸­çš„ç›®æ ‡è¯æ±‡æ•°é‡ï¼ˆä¿ç•™å¤§å°å†™ï¼‰
            tokens = simple_tokenize(current_segment)
            target_count = sum(1 for token in tokens if token in target_tokens)
            
            if target_count > current_target_count:
                current_target_count = target_count
            
            # æ›´æ–°æœ€ä½³ç‰‡æ®µ
            if current_target_count > max_target_count:
                max_target_count = current_target_count
                best_segment = current_segment
    
    return best_segment.strip(), max_target_count

# ============= æ·»åŠ ç¼“å­˜ç®¡ç†ç±» (åœ¨ VocabManager ç±»ä¹‹å‰) =============
class TextCacheManager:
    """ç®¡ç†æ–‡æœ¬ç¼“å­˜"""
    
    def __init__(self, cache_file='text_cache.txt'):
        self.cache_file = cache_file
    
    def save_text(self, text):
        """ä¿å­˜æ–‡æœ¬åˆ°ç¼“å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                f.write(text)
            return True
        except Exception as e:
            st.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def load_text(self):
        """ä»ç¼“å­˜åŠ è½½æ–‡æœ¬"""
        try:
            if Path(self.cache_file).exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return f.read()
            return None
        except Exception as e:
            st.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        try:
            if Path(self.cache_file).exists():
                Path(self.cache_file).unlink()
            return True
        except Exception as e:
            st.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def has_cache(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜"""
        return Path(self.cache_file).exists()



# æ•°æ®ç®¡ç†ç±»
class VocabManager:
    def __init__(self, vocab_file='vocab.json'):
        self.vocab_file = vocab_file
        self.vocab = self.load_vocab()
    
    def load_vocab(self):
        """åŠ è½½è¯æ±‡è¡¨"""
        try:
            with open(self.vocab_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def save_vocab(self):
        """ä¿å­˜è¯æ±‡è¡¨"""
        with open(self.vocab_file, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, ensure_ascii=False, indent=2)
    
    def add_word(self, word, text=""):
        """æ·»åŠ ç”Ÿè¯ï¼ˆä¿®å¤ç‰ˆï¼šé¿å…è¦†ç›–å·²æœ‰æ•°æ®ï¼‰"""
        if word not in self.vocab:
            self.vocab[word] = {
                "word": word,
                "added_date": datetime.now().isoformat(),
                "last_review": datetime.now().isoformat(),
                "status": "learning",
                "encounter_count": 0,
                "concordance": get_concordance(word, text) if text else []
            }
        else:
            # è¯æ±‡å·²å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯ä»ç†Ÿè¯é™çº§ï¼‰
            # åªæ›´æ–°concordanceå’Œstatusï¼Œä¸è¦†ç›–å…¶ä»–å­—æ®µ
            if text and not self.vocab[word].get("concordance"):
                self.vocab[word]["concordance"] = get_concordance(word, text)
            if self.vocab[word]["status"] == "familiar":
                # å¦‚æœæ˜¯ä»ç†Ÿè¯é™çº§ï¼Œé‡ç½®ä¸ºå­¦ä¹ çŠ¶æ€
                self.vocab[word]["status"] = "learning"
                self.vocab[word]["added_date"] = datetime.now().isoformat()
        
        return self.vocab[word]
    
    def mark_as_familiar(self, word):
        """æ ‡è®°ä¸ºç†Ÿè¯"""
        if word in self.vocab:
            self.vocab[word]["status"] = "familiar"
            self.vocab[word]["last_review"] = datetime.now().isoformat()
            self.save_vocab()
    
    def mark_as_learning(self, word):
        """é™çº§åˆ°ç”Ÿè¯ï¼ˆä¿®å¤ç‰ˆï¼šä¿ç•™å†å²ä¿¡æ¯ï¼‰"""
        if word in self.vocab:
            # ä¿ç•™åŸæœ‰ä¿¡æ¯ï¼Œåªä¿®æ”¹çŠ¶æ€å’Œæ—¶é—´æˆ³
            self.vocab[word]["status"] = "learning"
            self.vocab[word]["last_review"] = datetime.now().isoformat()
            # é‡æ–°è®¾ç½®æ·»åŠ æ—¥æœŸä¸ºä»Šå¤©ï¼ˆå¼€å¯æ–°çš„3å¤©å€’è®¡æ—¶ï¼‰
            self.vocab[word]["added_date"] = datetime.now().isoformat()
            self.save_vocab()
    
    def increment_encounter(self, word):
        """å¢åŠ é‡åˆ°æ¬¡æ•°"""
        if word in self.vocab:
            self.vocab[word]["encounter_count"] += 1
            self.save_vocab()
    
    def clean_expired_words(self, days=3):
        """åˆ é™¤è¶…è¿‡3å¤©æœªå¤ä¹ çš„ç”Ÿè¯"""
        cutoff = datetime.now() - timedelta(days=days)
        expired_words = []
        
        for word, meta in list(self.vocab.items()):
            if meta["status"] == "learning":
                added_date = datetime.fromisoformat(meta["added_date"])
                if added_date < cutoff:
                    expired_words.append(word)
                    del self.vocab[word]
        
        if expired_words:
            self.save_vocab()
        return expired_words
    
    def get_learning_words(self):
        """è·å–ç”Ÿè¯è¡¨"""
        return {k: v for k, v in self.vocab.items() if v["status"] == "learning"}
    
    def get_familiar_words(self):
        """è·å–ç†Ÿè¯è¡¨"""
        return {k: v for k, v in self.vocab.items() if v["status"] == "familiar"}
# ============= æ–°å¢æ•°æ®ç®¡ç†ç±» =============
class ReadingManager:
    """ç®¡ç†ä¹¦ç±å’Œé˜…è¯»ä¼šè¯æ•°æ®"""
    
    def __init__(self, db_file='reading_stats.db'):
        self.db_file = db_file
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # ä¹¦ç±è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS books (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT UNIQUE NOT NULL,
                author TEXT,
                added_date TEXT NOT NULL,
                reading_status TEXT DEFAULT 'reading'
            )
        ''')
        
        # é˜…è¯»ä¼šè¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS reading_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                book_id INTEGER NOT NULL,
                session_date TEXT NOT NULL,
                total_tokens INTEGER,
                new_words INTEGER,
                familiar_words_count INTEGER,
                duration_minutes INTEGER,
                notes TEXT,
                FOREIGN KEY(book_id) REFERENCES books(id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_book(self, title, author=""):
        """æ·»åŠ æ–°ä¹¦ç±"""
        try:
            conn = sqlite3.connect(self.db_file)
            cursor = conn.cursor()
            cursor.execute(
                'INSERT INTO books (title, author, added_date) VALUES (?, ?, ?)',
                (title, author, datetime.now().isoformat())
            )
            conn.commit()
            book_id = cursor.lastrowid
            conn.close()
            return book_id
        except sqlite3.IntegrityError:
            return None  # ä¹¦ç±å·²å­˜åœ¨
    
    def get_all_books(self):
        """è·å–æ‰€æœ‰ä¹¦ç±"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        cursor.execute('SELECT id, title, author, added_date, reading_status FROM books ORDER BY added_date DESC')
        books = cursor.fetchall()
        conn.close()
        return books
    
    def add_session(self, book_id, total_tokens, new_words, familiar_words_count, duration_minutes=0, notes=""):
        """æ·»åŠ é˜…è¯»ä¼šè¯"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        cursor.execute(
            '''INSERT INTO reading_sessions 
               (book_id, session_date, total_tokens, new_words, familiar_words_count, duration_minutes, notes)
               VALUES (?, ?, ?, ?, ?, ?, ?)''',
            (book_id, datetime.now().isoformat(), total_tokens, new_words, familiar_words_count, duration_minutes, notes)
        )
        conn.commit()
        conn.close()
    
    def get_book_stats(self, book_id):
        """è·å–å•æœ¬ä¹¦çš„ç»Ÿè®¡æ•°æ®"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # è·å–ä¹¦ç±ä¿¡æ¯
        cursor.execute('SELECT title, author, added_date FROM books WHERE id = ?', (book_id,))
        book_info = cursor.fetchone()
        
        # è·å–æ‰€æœ‰ä¼šè¯
        cursor.execute(
            'SELECT session_date, total_tokens, new_words, familiar_words_count, duration_minutes, notes FROM reading_sessions WHERE book_id = ? ORDER BY session_date',
            (book_id,)
        )
        sessions = cursor.fetchall()
        conn.close()
        
        if not book_info:
            return None
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_tokens = sum(s[1] for s in sessions)
        total_new_words = sum(s[2] for s in sessions)
        total_familiar_count = sum(s[3] for s in sessions)
        session_count = len(sessions)
        
        return {
            'title': book_info[0],
            'author': book_info[1],
            'added_date': book_info[2],
            'sessions': sessions,
            'total_tokens': total_tokens,
            'total_new_words': total_new_words,
            'total_familiar_count': total_familiar_count,
            'session_count': session_count,
            'avg_new_words_per_session': total_new_words / session_count if session_count > 0 else 0,
            'new_words_ratio': (total_new_words / total_tokens * 100) if total_tokens > 0 else 0
        }
    
    def get_all_stats(self):
        """è·å–æ‰€æœ‰ä¹¦ç±çš„ç»Ÿè®¡æ‘˜è¦"""
        books = self.get_all_books()
        stats_list = []
        
        for book_id, title, author, added_date, status in books:
            stats = self.get_book_stats(book_id)
            if stats:
                stats['book_id'] = book_id
                stats['status'] = status
                stats_list.append(stats)
        
        return stats_list
    
    def update_book_status(self, book_id, status):
        """æ›´æ–°ä¹¦ç±é˜…è¯»çŠ¶æ€"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        cursor.execute('UPDATE books SET reading_status = ? WHERE id = ?', (status, book_id))
        conn.commit()
        conn.close()

# åˆå§‹åŒ–
if 'vocab_manager' not in st.session_state:
    st.session_state.vocab_manager = VocabManager()
if 'current_text' not in st.session_state:
    st.session_state.current_text = ""
if 'readable_text' not in st.session_state:
    st.session_state.readable_text = ""
if 'new_tokens' not in st.session_state:
    st.session_state.new_tokens = []

vm = st.session_state.vocab_manager

if 'reading_manager' not in st.session_state:
    st.session_state.reading_manager = ReadingManager()
if 'current_book_id' not in st.session_state:
    st.session_state.current_book_id = None

rm = st.session_state.reading_manager

if 'text_cache_manager' not in st.session_state:
    st.session_state.text_cache_manager = TextCacheManager()

tcm = st.session_state.text_cache_manager

# Streamlit ç•Œé¢
st.title("- Little Fuga 578-zehn ")
st.set_page_config(
    page_title="Fuga Vocabs",
    page_icon="ğŸ’¿",
    layout="wide"
)

# ä¾§è¾¹æ ï¼šç»Ÿè®¡ä¿¡æ¯
with st.sidebar:
    st.header("ğŸ“Š è¯æ±‡ç»Ÿè®¡")
    learning_words = vm.get_learning_words()
    familiar_words = vm.get_familiar_words()
    
    st.metric("ç”Ÿè¯æ•°é‡", len(learning_words))
    st.metric("ç†Ÿè¯æ•°é‡", len(familiar_words))
    st.metric("æ€»è¯æ±‡é‡", len(vm.vocab))
    
    st.divider()
    
    # è‡ªåŠ¨æ¸…ç†è¿‡æœŸç”Ÿè¯
    if st.button("ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸç”Ÿè¯ï¼ˆ>3å¤©ï¼‰"):
        expired = vm.clean_expired_words(days=3)
        if expired:
            st.success(f"å·²åˆ é™¤ {len(expired)} ä¸ªè¿‡æœŸç”Ÿè¯")
            st.write(expired)
        else:
            st.info("æ²¡æœ‰è¿‡æœŸç”Ÿè¯")

# ä¸»é¡µé¢ - æ ‡ç­¾é¡µ
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸ“¥ æ–‡æœ¬è½½å…¥", 
    "ğŸ“– é˜…è¯»æ¨¡å¼", 
    "ğŸ“š ç”Ÿè¯å­¦ä¹ ", 
    "ğŸ´ Ankiå¤ä¹ ", 
    "ğŸ§ª æ¯å‘¨æŠ½æ£€",
    "ğŸ“Š é˜…è¯»ç»Ÿè®¡"
])

# Tab 1: æ–‡æœ¬è½½å…¥
with tab1:
    st.header("ğŸ“¥ æ–‡æœ¬è½½å…¥")
    
    # ============= æ·»åŠ ä¹¦ç±é€‰æ‹© =============
    # ã€æ–°å¢ã€‘ä¹¦ç±ç®¡ç†åŒºåŸŸ
    st.subheader("ğŸ“– é€‰æ‹©æˆ–æ·»åŠ ä¹¦ç±")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # è·å–æ‰€æœ‰ä¹¦ç±
        books = rm.get_all_books()
        book_options = {f"{b[1]} - {b[2]}" if b[2] else b[1]: b[0] for b in books}
        book_options["+ æ–°å¢ä¹¦ç±"] = None
        
        selected_book = st.selectbox(
            "é€‰æ‹©ä¹¦ç±ï¼š",
            options=list(book_options.keys()),
            key="book_selector"
        )
        
        if selected_book == "+ æ–°å¢ä¹¦ç±":
            st.session_state.current_book_id = None
        else:
            st.session_state.current_book_id = book_options[selected_book]
    
    with col2:
        if st.button("â• æ–°å¢ä¹¦ç±", key="add_book_btn"):
            st.session_state.show_add_book_form = True
    
    # æ–°å¢ä¹¦ç±è¡¨å•
    if st.session_state.get('show_add_book_form'):
        st.divider()
        with st.form("add_book_form"):
            new_title = st.text_input("ä¹¦ç±æ ‡é¢˜ *", placeholder="ä¾‹å¦‚ï¼šDer Steppenwolf")
            new_author = st.text_input("ä½œè€…ï¼ˆå¯é€‰ï¼‰", placeholder="ä¾‹å¦‚ï¼šHermann Hesse")
            
            if st.form_submit_button("âœ… æ·»åŠ ä¹¦ç±"):
                if new_title.strip():
                    book_id = rm.add_book(new_title.strip(), new_author.strip())
                    if book_id:
                        st.session_state.current_book_id = book_id
                        st.session_state.show_add_book_form = False
                        st.success(f"âœ… ä¹¦ç± '{new_title}' æ·»åŠ æˆåŠŸï¼")
                        st.rerun()
                    else:
                        st.error("âŒ è¯¥ä¹¦ç±å·²å­˜åœ¨")
                else:
                    st.error("è¯·è¾“å…¥ä¹¦ç±æ ‡é¢˜")
    
    st.divider()
    
    # æ˜¾ç¤ºå½“å‰é€‰ä¸­çš„ä¹¦ç±
    if st.session_state.current_book_id:
        books_dict = {b[0]: b[1] for b in rm.get_all_books()}
        current_title = books_dict.get(st.session_state.current_book_id)
        st.info(f"ğŸ“– å½“å‰é€‰ä¸­ï¼š**{current_title}**")
    
    
        
    st.subheader("ğŸ“ æ–‡æœ¬è¾“å…¥åŒºåŸŸ")

    # ç¼“å­˜çŠ¶æ€å’Œæ“ä½œæŒ‰é’®
    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        if tcm.has_cache():
            st.success("âœ… å·²ä¿å­˜ä¸Šä¸€æ¬¡çš„æ–‡æœ¬ç¼“å­˜")
        else:
            st.info("ğŸ’¡ è¿˜æœªä¿å­˜æ–‡æœ¬ç¼“å­˜")

    with col2:
        if tcm.has_cache():
            if st.button("â®ï¸ æ¢å¤ä¸Šæ¬¡æ–‡æœ¬", key="restore_cache"):
                cached_text = tcm.load_text()
                if cached_text:
                    st.session_state.text_input_value = cached_text
                    st.rerun()

    with col3:
        if tcm.has_cache():
            if st.button("ğŸ—‘ï¸ æ¸…é™¤ç¼“å­˜", key="clear_cache"):
                if tcm.clear_cache():
                    st.success("âœ… ç¼“å­˜å·²æ¸…é™¤")
                    st.rerun()

    st.divider()

    # æ–‡æœ¬è¾“å…¥æ¡†
    text_input = st.text_area(
        "ç²˜è´´æˆ–è¾“å…¥å¾·è¯­æ–‡æœ¬ï¼š",
        value=st.session_state.get('text_input_value', ''),
        height=300,
        placeholder="ä¾‹å¦‚ï¼šIch gehe heute in die Schule. Das Wetter ist sehr schÃ¶n..."
    )


    buffer_size = st.number_input("æ¯æ¬¡åŠ è½½ç”Ÿè¯æ•°é‡ï¼š", min_value=10, max_value=200, value=50)
   
    # åœ¨å¼€å¤´æ·»åŠ ä¿å­˜ç¼“å­˜çš„é€»è¾‘ï¼š
    if st.button("ğŸ”„ å¤„ç†æ–‡æœ¬", type="primary"):
        if text_input:
            # ä¿å­˜æ–‡æœ¬åˆ°ç¼“å­˜
            tcm.save_text(text_input)
            st.session_state.text_input_value = text_input
            expired = vm.clean_expired_words(days=3)

            # æ£€æŸ¥æ˜¯å¦é€‰äº†ä¹¦ç±
            if not st.session_state.current_book_id:
                st.error("âŒ è¯·å…ˆé€‰æ‹©æˆ–åˆ›å»ºä¸€æœ¬ä¹¦ç±")
            else:
                # ã€åŸæœ‰é€»è¾‘ã€‘
                expired = vm.clean_expired_words(days=3)
                if expired:
                    st.warning(f"âš ï¸ å·²è‡ªåŠ¨åˆ é™¤ {len(expired)} ä¸ªè¶…è¿‡3å¤©æœªå¤ä¹ çš„ç”Ÿè¯")
                
                tokens = simple_tokenize(text_input)
                st.session_state.current_text = text_input
                
                token_counts = Counter(tokens)
                familiar_set = set(vm.get_familiar_words().keys())
                learning_set = set(vm.get_learning_words().keys())
                
                new_words = []
                for word, count in token_counts.most_common():
                    if word not in familiar_set and word not in learning_set:
                        new_words.append(word)
                        if len(new_words) >= buffer_size:
                            break
                
                for word in familiar_set:
                    if word in token_counts:
                        vm.increment_encounter(word)
                
                for word in new_words:
                    vm.add_word(word, text_input)
                
                st.session_state.new_tokens = new_words
                vm.save_vocab()
                
                target_tokens = set(vm.get_learning_words().keys()) | set(vm.get_familiar_words().keys())
                readable_text, target_count = extract_readable_text(text_input, target_tokens)
                st.session_state.readable_text = readable_text
                
                # ã€æ–°å¢ã€‘è®°å½•åˆ°æ•°æ®åº“
                rm.add_session(
                    book_id=st.session_state.current_book_id,
                    total_tokens=len(tokens),
                    new_words=len(new_words),
                    familiar_words_count=target_count
                )
                
                st.success(f"âœ… å¤„ç†å®Œæˆï¼å‘ç° {len(new_words)} ä¸ªæ–°è¯")
                st.info(f"ğŸ“– ç”Ÿæˆé˜…è¯»æ–‡æœ¬ï¼ŒåŒ…å« {target_count} ä¸ªå·²å­¦è¯æ±‡")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æ€»è¯æ•°", len(tokens))
                with col2:
                    st.metric("å”¯ä¸€è¯æ•°", len(set(tokens)))
                with col3:
                    st.metric("æ–°è¯æ•°", len(new_words))
                with col4:
                    st.metric("å¯è¯»è¯æ•°", target_count)
        else:
            st.error("è¯·å…ˆè¾“å…¥æ–‡æœ¬ï¼")

# Tab 2: é˜…è¯»æ¨¡å¼
with tab2:
    st.header("ğŸ“– é˜…è¯»æ¨¡å¼")
    
    if not st.session_state.readable_text:
        st.info("ğŸ’¡ è¯·å…ˆåœ¨ã€Œæ–‡æœ¬è½½å…¥ã€é¡µé¢å¤„ç†æ–‡æœ¬")
    else:
        # è·å–ç›®æ ‡è¯æ±‡
        learning_set = set(vm.get_learning_words().keys())
        familiar_set = set(vm.get_familiar_words().keys())
        all_known = learning_set | familiar_set
        
        st.markdown("### ğŸ“„ ä¼˜åŒ–åçš„é˜…è¯»æ–‡æœ¬")
        st.markdown("---")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        tokens_in_text = simple_tokenize(st.session_state.readable_text)
        known_count = sum(1 for t in tokens_in_text if t in all_known)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ–‡æœ¬è¯æ•°", len(tokens_in_text))
        with col2:
            st.metric("å·²å­¦è¯æ•°", known_count)
        with col3:
            coverage = (known_count / len(tokens_in_text) * 100) if tokens_in_text else 0
            st.metric("è¦†ç›–ç‡", f"{coverage:.1f}%")
        
        st.divider()
        
        # é«˜äº®æ˜¾ç¤ºæ–‡æœ¬
        display_text = st.session_state.readable_text
        words_in_text = re.findall(r'\b[a-zA-ZÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]+\b', display_text)
        
        for word in set(words_in_text):
            if word in learning_set:
                # ç”Ÿè¯ç”¨çº¢è‰²é«˜äº®ï¼ˆä¿ç•™å¤§å°å†™ï¼‰
                display_text = re.sub(
                    rf'\b{re.escape(word)}\b',
                    f'<mark style="background-color: #ffcccb;">{word}</mark>',
                    display_text
                )
            elif word in familiar_set:
                # ç†Ÿè¯ç”¨ç»¿è‰²é«˜äº®ï¼ˆä¿ç•™å¤§å°å†™ï¼‰
                display_text = re.sub(
                    rf'\b{re.escape(word)}\b',
                    f'<mark style="background-color: #90EE90;">{word}</mark>',
                    display_text
                )
        
        st.markdown(display_text, unsafe_allow_html=True)
        
        st.divider()
        
        # å›¾ä¾‹
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("ğŸŸ¥ <mark style='background-color: #ffcccb;'>ç”Ÿè¯ï¼ˆéœ€è¦å­¦ä¹ ï¼‰</mark>", unsafe_allow_html=True)
        with col2:
            st.markdown("ğŸŸ© <mark style='background-color: #90EE90;'>ç†Ÿè¯ï¼ˆå·²æŒæ¡ï¼‰</mark>", unsafe_allow_html=True)
        
        # å¯¼å‡ºé€‰é¡¹
        st.divider()
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½é˜…è¯»æ–‡æœ¬",
            data=st.session_state.readable_text,
            file_name=f"reading_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain"
        )

# Tab 3: ç”Ÿè¯å­¦ä¹ 
with tab3:
    st.header("ğŸ“š ç”Ÿè¯å­¦ä¹ ")
    
    learning_words = vm.get_learning_words()
    
    if not learning_words:
        st.info("ğŸ‰ å½“å‰æ²¡æœ‰ç”Ÿè¯ï¼å»ã€Œæ–‡æœ¬è½½å…¥ã€é¡µé¢åŠ è½½æ–°æ–‡æœ¬å§ã€‚")
    else:
        st.write(f"**å½“å‰ç”Ÿè¯æ•°é‡ï¼š{len(learning_words)}**")
        
        # å¯¼å‡ºç”Ÿè¯è¡¨ï¼ˆç”¨äºè·å–ç¿»è¯‘ï¼‰
        st.divider()
        st.subheader("ğŸ“¤ æ­¥éª¤1ï¼šå¯¼å‡ºç”Ÿè¯è¡¨")
        
        # ğŸ†• è‡ªåŠ¨ç”Ÿæˆå¸¦æç¤ºè¯çš„å†…å®¹
        llm_prompt = "è¯·ç»™å‡ºä¸‹åˆ—å¾·æ–‡å•è¯çš„ç¿»è¯‘ï¼Œæ ¼å¼ä¸º Deutsch, ä¸­æ–‡ç¿»è¯‘ \n æ¯å€‹å–®è©ä¸€è¡Œ\n"
        word_list_text = "\n".join(learning_words.keys())
        full_text_with_prompt = llm_prompt + word_list_text
        
        col1, col2 = st.columns([3, 1])
        with col1:
            # æ˜¾ç¤ºå¸¦æç¤ºè¯çš„å®Œæ•´æ–‡æœ¬
            st.text_area(
                "å•è¯åˆ—è¡¨ï¼ˆå¤åˆ¶åç²˜è´´ç»™AIç¿»è¯‘ï¼‰ï¼š", 
                full_text_with_prompt, 
                height=200,
                disabled=False  # ä¸è¦è¨­ç½®åªè®€ï¼Œå› ç‚ºé€™æ¨£ç”¨æˆ¶é€£å¾©è£½éƒ½ä¸è¡Œ
            )
        with col2:
            # ä¸‹è½½é€‰é¡¹ï¼šåŒæ—¶ä¸‹è½½æç¤ºè¯å’Œå•è¯åˆ—è¡¨
            st.download_button(
                label="ğŸ’¾ ä¸‹è½½ TXTï¼ˆå«æç¤ºè¯ï¼‰",
                data=full_text_with_prompt,
                file_name=f"words_prompt_{datetime.now().strftime('%Y%m%d')}.txt",
                mime="text/plain"
            )
            
            st.download_button(
                label="ğŸ’¾ ä¸‹è½½å•è¯è¡¨",
                data=word_list_text,
                file_name=f"words_{datetime.now().strftime('%Y%m%d')}.txt",
                mime="text/plain"
            )
        
        st.info("ğŸ’¡ ç‚¹å‡»ã€Œå¤åˆ¶ã€æŒ‰é’®ï¼ˆåœ¨æ–‡æœ¬æ¡†å³ä¸Šè§’ï¼‰ï¼Œæˆ–ä¸‹è½½åç²˜è´´ç»™ Claude/ChatGPT")
        
        # å¯¼å…¥CSVé‡Šä¹‰
        st.divider()
        st.subheader("ğŸ“¥ æ­¥éª¤2ï¼šå¯¼å…¥å‚è€ƒé‡Šä¹‰")
        
        st.markdown("""
        **ç²˜è´´AIç”Ÿæˆçš„CSVè¡¨æ ¼**ï¼ˆæ ¼å¼ï¼š`å¾·è¯­å•è¯,ä¸­æ–‡é‡Šä¹‰`ï¼‰
        ```
        beispiel,ä¾‹å­
        schÃ¶n,ç¾ä¸½çš„
        lernen,å­¦ä¹ 
        ```
        """)
        
        csv_input = st.text_area(
            "ç²˜è´´CSVå†…å®¹ï¼š",
            height=200,
            placeholder="beispiel,ä¾‹å­\nschÃ¶n,ç¾ä¸½çš„\nlernen,å­¦ä¹ ",
            key="csv_import"
        )
        
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸ“ å¯¼å…¥é‡Šä¹‰", type="primary"):
                if csv_input.strip():
                    imported_count = 0
                    skipped_count = 0
                    
                    for line in csv_input.strip().split('\n'):
                        line = line.strip()
                        if not line or ',' not in line:
                            continue
                        
                        parts = line.split(',', 1)
                        if len(parts) != 2:
                            continue
                        
                        word = parts[0].strip()  # ä¿ç•™å¤§å°å†™
                        translation = parts[1].strip()
                        
                        if word in vm.vocab:
                            vm.vocab[word]["translation"] = translation
                            imported_count += 1
                        else:
                            skipped_count += 1
                    
                    vm.save_vocab()
                    st.success(f"âœ… æˆåŠŸå¯¼å…¥ {imported_count} ä¸ªé‡Šä¹‰")
                    if skipped_count > 0:
                        st.warning(f"âš ï¸ è·³è¿‡ {skipped_count} ä¸ªä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯")
                    st.rerun()
                else:
                    st.error("è¯·å…ˆç²˜è´´CSVå†…å®¹")
        
        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰é‡Šä¹‰"):
                for word in vm.vocab:
                    if "translation" in vm.vocab[word]:
                        del vm.vocab[word]["translation"]
                vm.save_vocab()
                st.success("å·²æ¸…é™¤æ‰€æœ‰é‡Šä¹‰")
                st.rerun()
        
        # æ˜¾ç¤ºç”Ÿè¯åˆ—è¡¨ï¼ˆå¸¦é‡Šä¹‰ï¼‰
        st.divider()
        st.subheader("ğŸ“‹ æ­¥éª¤3ï¼šç”Ÿè¯ä¸€è§ˆè¡¨")
        
        df_data = []
        for word, meta in learning_words.items():
            added_date = datetime.fromisoformat(meta["added_date"])
            days_left = 3 - (datetime.now() - added_date).days
            
            df_data.append({
                "å•è¯": word,
                "å‚è€ƒé‡Šä¹‰": meta.get("translation", "âŒ æœªå¯¼å…¥"),
                "æ·»åŠ æ—¥æœŸ": added_date.strftime("%Y-%m-%d"),
                "å‰©ä½™å¤©æ•°": f"{days_left} å¤©",
                "ä¾‹å¥æ•°é‡": len(meta.get("concordance", []))
            })
        
        df = pd.DataFrame(df_data)
        st.dataframe(df, use_container_width=True)
        
        # ç»Ÿè®¡é‡Šä¹‰è¦†ç›–ç‡
        with_translation = sum(1 for meta in learning_words.values() if meta.get("translation"))
        coverage = (with_translation / len(learning_words) * 100) if learning_words else 0
        st.metric("é‡Šä¹‰è¦†ç›–ç‡", f"{coverage:.1f}% ({with_translation}/{len(learning_words)})")

# Tab 4: Anki å¤ä¹ 
with tab4:
    st.header("ğŸ´ ç®€æ˜“ Anki å¤ä¹ ")
    
    learning_words = vm.get_learning_words()
    
    if not learning_words:
        st.info("å½“å‰æ²¡æœ‰éœ€è¦å¤ä¹ çš„ç”Ÿè¯ã€‚")
    else:
        st.write(f"**å¾…å¤ä¹ ç”Ÿè¯ï¼š{len(learning_words)} ä¸ª**")
        
        # åˆå§‹åŒ–å¤ä¹ ç´¢å¼•
        if 'review_index' not in st.session_state:
            st.session_state.review_index = 0
            st.session_state.review_words = list(learning_words.keys())
            random.shuffle(st.session_state.review_words)
        
        if st.session_state.review_index < len(st.session_state.review_words):
            current_word = st.session_state.review_words[st.session_state.review_index]
            word_meta = learning_words[current_word]
            
            # æ˜¾ç¤ºå½“å‰å•è¯
            st.markdown(f"### å•è¯ {st.session_state.review_index + 1}/{len(st.session_state.review_words)}")
            st.markdown(f"# **{current_word}**")
            
            # æ˜¾ç¤ºå‚è€ƒé‡Šä¹‰ï¼ˆå¯æŠ˜å ï¼‰
            if word_meta.get("translation"):
                with st.expander("ğŸ’­ æŸ¥çœ‹å‚è€ƒé‡Šä¹‰", expanded=False):
                    st.markdown(f"### {word_meta['translation']}")
            else:
                st.caption("ğŸ’¡ æç¤ºï¼šå¯åœ¨ã€Œç”Ÿè¯å­¦ä¹ ã€é¡µé¢å¯¼å…¥é‡Šä¹‰")
            
            # æ˜¾ç¤ºä¾‹å¥
            if word_meta.get("concordance"):
                with st.expander("ğŸ“– æŸ¥çœ‹ä¾‹å¥", expanded=False):
                    for i, context in enumerate(word_meta["concordance"], 1):
                        st.write(f"{i}. ...{context}...")
            
            st.divider()
            
            # æ“ä½œæŒ‰é’®
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("âœ… è®¤è¯† â†’ ç†Ÿè¯è¡¨", use_container_width=True, type="primary"):
                    vm.mark_as_familiar(current_word)
                    st.session_state.review_index += 1
                    st.rerun()
            
            with col2:
                if st.button("âŒ ä¸è®¤è¯† â†’ è·³è¿‡", use_container_width=True):
                    st.session_state.review_index += 1
                    st.rerun()
            

            
            # è¿›åº¦æ¡
            progress = st.session_state.review_index / len(st.session_state.review_words)
            st.progress(progress)
        else:
            st.success("ğŸ‰ æœ¬è½®å¤ä¹ å®Œæˆï¼")
            if st.button("ğŸ”„ é‡æ–°å¼€å§‹å¤ä¹ "):
                st.session_state.review_index = 0
                st.session_state.review_words = list(learning_words.keys())
                random.shuffle(st.session_state.review_words)
                st.rerun()

# Tab 5: æ¯å‘¨æŠ½æ£€
with tab5:
    st.header("ğŸ§ª ç†Ÿè¯æŠ½æ£€")
    
    familiar_words = vm.get_familiar_words()
    
    if not familiar_words:
        st.info("å½“å‰æ²¡æœ‰ç†Ÿè¯å¯ä»¥æŠ½æ£€ã€‚")
    else:
        st.write(f"**ç†Ÿè¯æ€»æ•°ï¼š{len(familiar_words)} ä¸ª**")
        
        test_ratio = st.slider("æŠ½æ£€æ¯”ä¾‹ï¼ˆ%ï¼‰ï¼š", 10, 100, 30)
        test_count = max(1, int(len(familiar_words) * test_ratio / 100))
        
        if st.button("ğŸ² å¼€å§‹éšæœºæŠ½æ£€", type="primary"):
            st.session_state.test_words = random.sample(list(familiar_words.keys()), test_count)
            st.session_state.test_index = 0
        
        st.divider()
        
        # æŠ½æ£€æµç¨‹
        if 'test_words' in st.session_state and st.session_state.test_words:
            if st.session_state.test_index < len(st.session_state.test_words):
                current_word = st.session_state.test_words[st.session_state.test_index]
                word_meta = familiar_words[current_word]
                
                st.markdown(f"### å•è¯ {st.session_state.test_index + 1}/{len(st.session_state.test_words)}")
                st.markdown(f"# **{current_word}**")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("é‡åˆ°æ¬¡æ•°", word_meta.get("encounter_count", 0))
                with col2:
                    added = datetime.fromisoformat(word_meta["added_date"])
                    days_ago = (datetime.now() - added).days
                    st.metric("å­¦ä¹ å¤©æ•°", f"{days_ago} å¤©")
                
                st.divider()
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("âœ… è®¤è¯†", use_container_width=True, type="primary"):
                        st.session_state.test_index += 1
                        st.rerun()
                
                with col2:
                    if st.button("âŒ ä¸è®¤è¯† â†’ é™çº§åˆ°ç”Ÿè¯", use_container_width=True):
                        vm.mark_as_learning(current_word)
                        st.session_state.test_index += 1
                        st.rerun()
                
                progress = st.session_state.test_index / len(st.session_state.test_words)
                st.progress(progress)
            else:
                st.success("ğŸ‰ æŠ½æ£€å®Œæˆï¼")
                if st.button("ğŸ”„ é‡æ–°æŠ½æ£€"):
                    del st.session_state.test_words
                    st.rerun()

# ============= æ–°å¢ Tab 6ï¼ˆé˜…è¯»ç»Ÿè®¡ï¼‰ =============
with tab6:
    st.header("ğŸ“Š é˜…è¯»ç»Ÿè®¡")
    
    all_stats = rm.get_all_stats()
    
    if not all_stats:
        st.info("ğŸ“š è¿˜æ²¡æœ‰é˜…è¯»è®°å½•ã€‚è¯·å…ˆåœ¨ã€Œæ–‡æœ¬è½½å…¥ã€é¡µé¢åŠ è½½æ–‡æœ¬ã€‚")
    else:
        # æ€»ä½“ç»Ÿè®¡å¡ç‰‡
        st.subheader("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
        
        total_books = len(all_stats)
        total_tokens_all = sum(s['total_tokens'] for s in all_stats)
        total_new_words_all = sum(s['total_new_words'] for s in all_stats)
        total_sessions = sum(s['session_count'] for s in all_stats)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ä¹¦ç±æ€»æ•°", total_books)
        with col2:
            st.metric("æ€»é˜…è¯»è¯æ•°", total_tokens_all)
        with col3:
            st.metric("æ€»ç”Ÿè¯æ•°", total_new_words_all)
        with col4:
            st.metric("æ€»ä¼šè¯æ•°", total_sessions)
        
        st.divider()
        
        # æŒ‰ä¹¦ç±è¯¦ç»†ç»Ÿè®¡
        st.subheader("ğŸ“š æŒ‰ä¹¦ç±ç»Ÿè®¡")
        
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        df_stats = []
        for stat in all_stats:
            df_stats.append({
                "ä¹¦ç±": stat['title'],
                "ä½œè€…": stat['author'] or "-",
                "é˜…è¯»è¯æ•°": stat['total_tokens'],
                "ç”Ÿè¯æ•°": stat['total_new_words'],
                "ç†Ÿè¯æ•°": stat['total_familiar_count'],
                "ä¼šè¯æ•°": stat['session_count'],
                "æ–°è¯ç‡": f"{stat['new_words_ratio']:.1f}%",
                "å¹³å‡æ–°è¯/ä¼šè¯": f"{stat['avg_new_words_per_session']:.1f}"
            })
        
        df = pd.DataFrame(df_stats)
        st.dataframe(df, use_container_width=True)
        
        st.divider()
        
        # å•æœ¬ä¹¦è¯¦ç»†ä¿¡æ¯
        st.subheader("ğŸ” å•æœ¬ä¹¦è¯¦æƒ…")
        
        book_titles = [s['title'] for s in all_stats]
        selected_stat = st.selectbox("é€‰æ‹©ä¹¦ç±ï¼š", book_titles)
        
        selected_book_stat = next(s for s in all_stats if s['title'] == selected_stat)
        
        # æ˜¾ç¤ºè¯¥ä¹¦çš„è¯¦ç»†æ•°æ®
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»é˜…è¯»è¯æ•°", selected_book_stat['total_tokens'])
        with col2:
            st.metric("æ€»ç”Ÿè¯æ•°", selected_book_stat['total_new_words'])
        with col3:
            st.metric("æ–°è¯ç‡", f"{selected_book_stat['new_words_ratio']:.1f}%")
        with col4:
            st.metric("é˜…è¯»ä¼šè¯æ•°", selected_book_stat['session_count'])
        
        st.divider()
        
        # è¯¥ä¹¦çš„æ‰€æœ‰ä¼šè¯è®°å½•
        st.markdown("### ğŸ“ é˜…è¯»ä¼šè¯è®°å½•")
        
        session_data = []
        for i, session in enumerate(selected_book_stat['sessions'], 1):
            session_date = datetime.fromisoformat(session[0])
            session_data.append({
                "åºå·": i,
                "æ—¥æœŸ": session_date.strftime("%Y-%m-%d %H:%M"),
                "è¯æ•°": session[1],
                "æ–°è¯": session[2],
                "ç†Ÿè¯": session[3],
                "æ—¶é•¿(åˆ†)": session[4] or "-",
                "ç¬”è®°": session[5] or "-"
            })
        
        df_sessions = pd.DataFrame(session_data)
        st.dataframe(df_sessions, use_container_width=True)
        
        # å¯¼å‡ºåŠŸèƒ½
        st.divider()
        st.markdown("### ğŸ’¾ å¯¼å‡ºæ•°æ®")
        
        export_df = pd.DataFrame(df_stats)
        csv_data = export_df.to_csv(index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½é˜…è¯»ç»Ÿè®¡ (CSV)",
            data=csv_data,
            file_name=f"reading_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )


# åº•éƒ¨ä¿¡æ¯
st.divider()
st.caption("ğŸ’¡ æç¤ºï¼šç”Ÿè¯ä¼šåœ¨3å¤©åè‡ªåŠ¨åˆ é™¤ï¼Œè¯·åŠæ—¶å¤ä¹ ï¼ç†Ÿè¯å¯é€šè¿‡æŠ½æ£€é™çº§å›ç”Ÿè¯è¡¨ã€‚")